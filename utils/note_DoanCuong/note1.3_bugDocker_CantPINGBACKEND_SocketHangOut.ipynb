{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "V·∫´n bug: POST http://127.0.0.1:25039/chat\n",
    "Error: socket hang up\n",
    "Request Headers\n",
    "Content-Type: application/json\n",
    "User-Agent: PostmanRuntime/7.43.0\n",
    "Accept: */*\n",
    "Cache-Control: no-cache\n",
    "Postman-Token: 91246568-07d1-4d86-8044-ef2e1d2e0853\n",
    "Host: 127.0.0.1:25039\n",
    "Accept-Encoding: gzip, deflate, br\n",
    "Connection: keep-alive\n",
    "Request Body\n",
    "\n",
    "\n",
    "\n",
    "Check @backend \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "OST http://127.0.0.1:25039/chat\n",
    "Error: connect ECONNREFUSED 127.0.0.1:25039\n",
    "Request Headers\n",
    "Content-Type: application/json\n",
    "User-Agent: PostmanRuntime/7.43.0\n",
    "Accept: */*\n",
    "Cache-Control: no-cache\n",
    "Postman-Token: ab0c86dd-e028-4539-aa5b-c5980abbbd19\n",
    "Host: 127.0.0.1:25039\n",
    "Accept-Encoding: gzip, deflate, br\n",
    "Connection: keep-alive\n",
    "Request Body\n",
    "\n",
    "\n",
    "BUG \n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "ed                                                                                                                                                  \n",
    "backend-1   | 2024-12-19 15:45:06,912 - WARNING - Compute Engine Metadata server unavailable on attempt 2 of 3. Reason: [Errno 111] Connection refused\n",
    "backend-1   | 2024-12-19 15:45:08,990 - WARNING - Compute Engine Metadata server unavailable on attempt 3 of 3. Reason: [Errno 111] Connection refused\n",
    "backend-1   | 2024-12-19 15:45:08,991 - WARNING - Authentication failed using Compute Engine authentication due to unavailable metadata server.\n",
    "backend-1   | Traceback (most recent call last):                                                                                                    \n",
    "backend-1   |   File \"/app/app.py\", line 70, in <module>\n",
    "backend-1   |     initialize_components()                                                                                                           \n",
    "backend-1   |   File \"/app/app.py\", line 51, in initialize_components                                                                               \n",
    "backend-1   |     llm_handler = LLMHandler(                                                                                                         \n",
    "backend-1   |   File \"/app/rag_pipeline/back.py\", line 37, in __init__\n",
    "backend-1   |     self.llm = ChatGoogleGenerativeAI(model=model_name, api_key=self.api_key)                                                         \n",
    "backend-1   |   File \"/usr/local/lib/python3.9/site-packages/langchain_core/load/serializable.py\", line 125, in __init__                            \n",
    "backend-1   |     super().__init__(*args, **kwargs)                                                                                                 \n",
    "backend-1   |   File \"/usr/local/lib/python3.9/site-packages/pydantic/main.py\", line 214, in __init__                                               \n",
    "backend-1   |     validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n",
    "backend-1   |   File \"/usr/local/lib/python3.9/site-packages/langchain_google_genai/chat_models.py\", line 838, in validate_environment              \n",
    "backend-1   |     self.client = genaix.build_generative_service(                                                                                    \n",
    "backend-1   |   File \"/usr/local/lib/python3.9/site-packages/langchain_google_genai/_genai_extension.py\", line 276, in build_generative_service     \n",
    "backend-1   |     return v1betaGenerativeServiceClient(**config)\n",
    "backend-1   |   File \"/usr/local/lib/python3.9/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 687, in __init__                                                                                                                                          \n",
    "backend-1   |     self._transport = transport_init(\n",
    "backend-1   |   File \"/usr/local/lib/python3.9/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py\", line 154, in __init__                                                                                                                                 \n",
    "backend-1   |     super().__init__(\n",
    "backend-1   |   File \"/usr/local/lib/python3.9/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/base.py\", line 100, in __init__                                                                                                                                 \n",
    "backend-1   |     credentials, _ = google.auth.default(\n",
    "backend-1   |   File \"/usr/local/lib/python3.9/site-packages/google/auth/_default.py\", line 697, in default                                         \n",
    "backend-1   |     raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)                                                          \n",
    "backend-1   | google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.                                                            \n",
    "backend-1 exited with code 0\n",
    "backend-1   | /app/rag_pipeline/back.py:43: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
    "backend-1   |   self.embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "backend-1   | 2024-12-19 15:45:16,270 - INFO - üîÑ Initializing components...                                                                        \n",
    "backend-1   | 2024-12-19 15:45:16,445 - INFO - Use pytorch device_name: cpu\n",
    "backend-1   | 2024-12-19 15:45:16,445 - INFO - Load pretrained SentenceTransformer: hiieu/halong_embedding\n",
    "backend-1   | /app/rag_pipeline/back.py:56: LangChainDeprecationWarning: The class `Qdrant` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-qdrant package and should be used instead. To use it run `pip install -U :class:`~langchain-qdrant` and import as `from :class:`~langchain_qdrant import Qdrant``.\n",
    "backend-1   |   return Qdrant(\n",
    "backend-1   | 2024-12-19 15:45:20,513 - INFO - ‚úÖ Vector database initialized                                                                       \n",
    "backend-1   | 2024-12-19 15:45:20,520 - WARNING - Compute Engine Metadata server unavailable on attempt 1 of 3. Reason: [Errno 111] Connection refused                                                                                                                                                  \n",
    "backend-1   | 2024-12-19 15:45:21,528 - WARNING - Compute Engine Metadata server unavailable on attempt 2 of 3. Reason: [Errno 111] Connection refused\n",
    "backend-1   | 2024-12-19 15:45:23,501 - WARNING - Compute Engine Metadata server unavailable on attempt 3 of 3. Reason: [Errno 111] Connection refused\n",
    "backend-1   | 2024-12-19 15:45:23,501 - WARNING - Authentication failed using Compute Engine authentication due to unavailable metadata server.\n",
    "backend-1   | Traceback (most recent call last):                                                                                                    \n",
    "backend-1   |   File \"/app/app.py\", line 70, in <module>\n",
    "backend-1   |     initialize_components()                                                                                                           \n",
    "backend-1   |   File \"/app/app.py\", line 51, in initialize_components                                                                               \n",
    "backend-1   |     llm_handler = LLMHandler(\n",
    "backend-1   |   File \"/app/rag_pipeline/back.py\", line 37, in __init__                                                                              \n",
    "backend-1   |     self.llm = ChatGoogleGenerativeAI(model=model_name, api_key=self.api_key)                                                         \n",
    "backend-1   |   File \"/usr/local/lib/python3.9/site-packages/langchain_core/load/serializable.py\", line 125, in __init__                            \n",
    "backend-1   |     super().__init__(*args, **kwargs)\n",
    "backend-1   |   File \"/usr/local/lib/python3.9/site-packages/pydantic/main.py\", line 214, in __init__                                               \n",
    "backend-1   |     validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)                                            \n",
    "backend-1   |   File \"/usr/local/lib/python3.9/site-packages/langchain_google_genai/chat_models.py\", line 838, in validate_environment\n",
    "backend-1   |     self.client = genaix.build_generative_service(                                                                                    \n",
    "backend-1   |   File \"/usr/local/lib/python3.9/site-packages/langchain_google_genai/_genai_extension.py\", line 276, in build_generative_service     \n",
    "backend-1   |     return v1betaGenerativeServiceClient(**config)                                                                                    \n",
    "backend-1   |   File \"/usr/local/lib/python3.9/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 687, in __init__                                                                                                                                          \n",
    "backend-1   |     self._transport = transport_init(\n",
    "backend-1   |   File \"/usr/local/lib/python3.9/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py\", line 154, in __init__                                                                                                                                 \n",
    "backend-1   |     super().__init__(\n",
    "backend-1   |   File \"/usr/local/lib/python3.9/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/base.py\", line 100, in __init__                                                                                                                                 \n",
    "backend-1   |     credentials, _ = google.auth.default(\n",
    "backend-1   |   File \"/usr/local/lib/python3.9/site-packages/google/auth/_default.py\", line 697, in default                                         \n",
    "backend-1   |     raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)\n",
    "backend-1   | google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.                                                            \n",
    "backend-1 exited with code 0\n",
    "backend-1   | /app/rag_pipeline/back.py:43: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
    "backend-1   |   self.embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "backend-1   | 2024-12-19 15:45:31,649 - INFO - üîÑ Initializing components...                                                                        \n",
    "backend-1   | 2024-12-19 15:45:31,842 - INFO - Use pytorch device_name: cpu                                                                         \n",
    "backend-1   | 2024-12-19 15:45:31,843 - INFO - Load pretrained SentenceTransformer: hiieu/halong_embedding\n",
    "                                                                                                                                                    \n",
    "\n",
    "v View in Docker Desktop   o View Config   w Enable Watch\n",
    "```\n",
    "\n",
    "Bug: X√ÅC TH·ª∞C GOOGLE G√å ƒê√ì L√ÄM CHO ƒê√ìNG DOCKER KH√îNG PING ƒê∆Ø·ª¢C BACKEND"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T√¥i s·∫Ω t√≥m t·∫Øt c√°c ƒëi·ªÉm ch√≠nh v·ªÅ l·ªói \"socket hang up\" t·ª´ c√°c c√¢u tr·∫£ l·ªùi tr√™n Stack Overflow:\n",
    "\n",
    "**Nguy√™n nh√¢n ph·ªï bi·∫øn:**\n",
    "\n",
    "1. **V·∫•n ƒë·ªÅ HTTP/HTTPS:**\n",
    "- ·ª®ng d·ª•ng ƒëang ch·∫°y HTTPS nh∆∞ng request g·ª≠i qua HTTP\n",
    "- C·∫ßn ƒë·∫£m b·∫£o protocol (http/https) kh·ªõp gi·ªØa client v√† server\n",
    "\n",
    "2. **V·∫•n ƒë·ªÅ Port:**\n",
    "- Port ƒëang ƒë∆∞·ª£c s·ª≠ d·ª•ng b·ªüi service kh√°c\n",
    "- Port b·ªã block ho·∫∑c kh√¥ng ƒë∆∞·ª£c support b·ªüi VPN/Proxy\n",
    "- N√™n th·ª≠ ƒë·ªïi port (v√≠ d·ª• t·ª´ 8088 sang 8089)\n",
    "\n",
    "3. **V·∫•n ƒë·ªÅ Network:**\n",
    "- VPN ƒëang b·∫≠t c√≥ th·ªÉ g√¢y ra v·∫•n ƒë·ªÅ\n",
    "- Proxy settings kh√¥ng ph√π h·ª£p\n",
    "- Network isolation trong Docker\n",
    "\n",
    "4. **V·∫•n ƒë·ªÅ Timeout:**\n",
    "- Request ch·ªù response qu√° l√¢u\n",
    "- C·∫ßn c·∫•u h√¨nh timeout ph√π h·ª£p\n",
    "- C√≥ th·ªÉ s·ª≠ d·ª•ng HTTP 202 (Accepted) ƒë·ªÉ handle long-running tasks\n",
    "\n",
    "5. **Headers v√† Content:**\n",
    "- Thi·∫øu Content-Length header\n",
    "- Content-Type kh√¥ng ph√π h·ª£p\n",
    "- Host header c√≥ v·∫•n ƒë·ªÅ\n",
    "\n",
    "**Gi·∫£i ph√°p cho Docker:**\n",
    "\n",
    "1. Ki·ªÉm tra v√† c·∫•u h√¨nh network trong Docker:\n",
    "```yaml\n",
    "services:\n",
    "  backend:\n",
    "    networks:\n",
    "      - app_network\n",
    "    ports:\n",
    "      - \"3000:3000\"\n",
    "```\n",
    "\n",
    "2. C·∫•u h√¨nh resources limits:\n",
    "```yaml\n",
    "services:\n",
    "  backend:\n",
    "    deploy:\n",
    "      resources:\n",
    "        limits:\n",
    "          memory: 2G\n",
    "          cpus: '2'\n",
    "```\n",
    "\n",
    "3. Th√™m health check:\n",
    "```yaml\n",
    "services:\n",
    "  backend:\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:3000/health\"]\n",
    "      interval: 30s\n",
    "      timeout: 10s\n",
    "      retries: 3\n",
    "```\n",
    "\n",
    "4. Ki·ªÉm tra logs Docker ƒë·ªÉ debug chi ti·∫øt h∆°n\n",
    "\n",
    "ƒê√¢y l√† nh·ªØng gi·∫£i ph√°p ph·ªï bi·∫øn nh·∫•t ƒë·ªÉ x·ª≠ l√Ω l·ªói \"socket hang up\" trong m√¥i tr∆∞·ªùng Docker.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
